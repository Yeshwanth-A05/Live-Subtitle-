Englishâ€“Tamil Neural Machine Translation



A deep learningâ€“based Englishâ€“Tamil NMT system comparing Seq2Seq with Attention and Transformer architectures for low-resource translation.

ğŸ“Œ Overview

Improves translation quality using attention mechanisms, subword tokenization, and Transformer-based models. Performance evaluated using BLEU and chrF metrics.

ğŸ§  Models

Seq2Seq (Encoderâ€“Decoder)

Attention Mechanism

Transformer (Self-Attention)

Subword Tokenization

Word Embeddings

âš™ï¸ Workflow

Data â†’ Preprocessing â†’ Tokenization â†’ Training â†’ Evaluation â†’ Inference

ğŸ›  Tech Stack

Python 3.8+ | PyTorch / TensorFlow | Transformers | NumPy | Pandas | NLTK | spaCy
