ğŸ“˜ A Comparative Analysis of Deep Learning Techniques for Englishâ€“Tamil Text Translation
ğŸ“Œ Overview

This project presents a comparative study of deep learning models for Englishâ€“Tamil Neural Machine Translation (NMT). The goal is to improve translation accuracy for the low-resource Englishâ€“Tamil language pair using modern deep learning techniques such as Seq2Seq with Attention and Transformer architectures.

The project focuses on improving translation quality for applications in education, government, and healthcare domains.

ğŸ¯ Objectives

Study various deep learning models for Englishâ€“Tamil translation

Compare performance of Seq2Seq and Transformer-based models

Implement attention mechanisms for better alignment

Improve translation quality using subword tokenization

Evaluate models using BLEU and chrF scores

â— Problem Statement

Despite advancements in Neural Machine Translation, Englishâ€“Tamil translation remains challenging due to:

Structural and grammatical differences

Rich morphology of Tamil

Limited parallel corpora

Vocabulary sparsity and rare word problems

Inaccurate translations affect usability in real-world domains such as education and healthcare.

ğŸ§  Techniques Used

ğŸ”¹ Sequence-to-Sequence (Seq2Seq) Architecture

ğŸ”¹ Attention Mechanism

ğŸ”¹ Transformer Architecture (Self-Attention)

ğŸ”¹ Subword Tokenization (to handle rare words)

ğŸ”¹ Word Embeddings

ğŸ—ï¸ Project Architecture

Workflow:

Data Acquisition
        â†“
Data Preprocessing (Cleaning, Tokenization, Padding)
        â†“
Train-Test Split
        â†“
Model Training (Transformer / Seq2Seq + Attention)
        â†“
Evaluation (BLEU, chrF)
        â†“
Inference (Tamil Translation Output)

ğŸ§© Modules

Data Preparation Module

Text cleaning

Normalization

Subword segmentation

Text Representation Module

Tokenization

Embedding generation

Model Training Module

Transformer-based encoder-decoder

Attention mechanism

Performance Evaluation Module

BLEU Score

chrF Score

Translation Inference Module

Generate Tamil translations

Output Refinement Module

Detokenization

Final formatting

ğŸ“Š Evaluation Metrics

Sentence-level BLEU

Corpus-level BLEU

chrF score

These metrics measure translation quality and alignment accuracy.

ğŸ’» Software Requirements

Python 3.8+

PyTorch 1.9+ / TensorFlow 2.x

Transformers Library

NumPy

Pandas

Scikit-learn

Matplotlib

NLTK / spaCy

ğŸ–¥ï¸ Hardware Requirements

GPU (NVIDIA GTX or better, 8GB+ VRAM recommended)

16GB RAM

Intel i7 / AMD Ryzen 7 (8+ cores)

SSD storage

ğŸ“š Literature Reference

Key foundational works include:

Bahdanau et al. (2014) â€“ Attention Mechanism

Sutskever et al. (2014) â€“ Seq2Seq with LSTM

Vaswani et al. (2017) â€“ Transformer Architecture

Conneau & Lample (2019) â€“ Pre-training for NMT

Tamil-English low-resource NMT studies (2020+)

ğŸŒ Social Impact

This project supports:

SDG 4 â€“ Quality Education

SDG 10 â€“ Reduced Inequalities

It promotes language accessibility and digital inclusion for Tamil-speaking communities.

ğŸš€ Future Improvements

Fine-tuning multilingual pretrained models (IndicBERT, mBART)

Increasing dataset size

Synthetic data generation

Domain-specific adaptation (medical/legal translation)

Low-resource transfer learning

ğŸ‘¥ Team Members

Saliniyan P â€“ 22ALR083

Sanjay A â€“ 22ALR084

Yeshwanth A â€“ 22ALR115

Project Guide:
Dr. K. Logeswaran
